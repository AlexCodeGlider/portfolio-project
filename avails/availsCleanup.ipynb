{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T16:54:54.205740Z",
     "start_time": "2021-01-09T16:54:50.292687Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "\n",
    "def tidy_split(df, column, sep=',', keep=False):\n",
    "    \"\"\"\n",
    "    Split the values of a column and expand so the new DataFrame has one split\n",
    "    value per row. Filters rows where the column is missing.\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "    df : pandas.DataFrame\n",
    "        dataframe with the column to split and expand\n",
    "    column : str\n",
    "        the column to split and expand\n",
    "    sep : str\n",
    "        the string used to split the column's values\n",
    "    keep : bool\n",
    "        whether to retain the presplit value as it's own row\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Returns a dataframe with the same columns as `df`.\n",
    "    \"\"\"\n",
    "    indexes = list()\n",
    "    new_values = list()\n",
    "    df = df.dropna(subset=[column])\n",
    "    for i, presplit in enumerate(df[column].astype(str)):\n",
    "        values = presplit.split(sep)\n",
    "        if keep and len(values) > 1:\n",
    "            indexes.append(i)\n",
    "            new_values.append(presplit)\n",
    "        for value in values:\n",
    "            indexes.append(i)\n",
    "            new_values.append(value.strip())\n",
    "    new_df = df.iloc[indexes, :].copy()\n",
    "    new_df[column] = new_values\n",
    "    return new_df\n",
    "\n",
    "def cleanup(df, suffix):\n",
    "    df['Non-Exclusive Date'] = df['Non-Exclusive Date'].replace('NOT AVAIL', np.nan).astype('datetime64')\n",
    "    df['Non-Exclusive Date'] = df.apply(lambda x: dt.date.today() if x['Available?'] == 'Avail NE' else x['Non-Exclusive Date'], axis=1)\n",
    "    max_prev_sale_enddate = df['Previous Sale Activity'].str.extractall(r'(\\d{2}[-]\\w{3}[-]\\d{4})').astype('datetime64').reset_index().groupby('level_0')[0].max()\n",
    "    max_prev_sale_enddate = max_prev_sale_enddate + pd.DateOffset(1)\n",
    "    df['max_prev_sale_enddate'] = max_prev_sale_enddate\n",
    "    df['Exclusive Date'] = df['Exclusive Date'].replace(['NOT AVAIL', 'NOT ACQ'], np.nan).astype('datetime64')\n",
    "    max_prev_sale_enddate = df[['Exclusive Date', 'max_prev_sale_enddate']].max(axis=1)\n",
    "    mask = ~df['Exclusive Date'].isna()\n",
    "    df.loc[mask, 'Exclusive Date'] = max_prev_sale_enddate\n",
    "    mask = df['Holdback'] <= dt.datetime.today()\n",
    "    df['Holdback'].loc[mask] = pd.NaT\n",
    "    mask = (df['Non-Exclusive Date'] < df['Exclusive Date']) & (df['Non-Exclusive Date'] > dt.datetime.today())\n",
    "    df['Available?'].loc[mask] = df['Non-Exclusive Date'].loc[mask]\n",
    "    \n",
    "    sale_activity = tidy_split(df, 'Previous Sale Activity', sep='\\n', keep=False)\n",
    "    sale_activity['Previous Sale Activity'] = sale_activity['Previous Sale Activity'].str.replace('.', '')\n",
    "    sale_activity_enddates = sale_activity['Previous Sale Activity'].str[-11:]\n",
    "    date_dict = {'ene': '01', 'feb': '02', 'mar': '03', 'abr': '04', 'may':'05', 'jun': '06', 'jul': '07', 'ago': '08', 'sep':'09', 'oct': '10', 'nov': '11', 'dic': '12'}\n",
    "    sale_activity_enddates = sale_activity_enddates.replace(date_dict, regex=True)\n",
    "    sale_activity_enddates = sale_activity_enddates.replace('own-Unknown', np.nan).astype('datetime64')\n",
    "    sale_activity['end_dates'] = sale_activity_enddates\n",
    "    sale_activity['end_dates'].fillna(sale_activity['Acq. Expires'], inplace=True)\n",
    "    sale_activity['client'] = sale_activity['Previous Sale Activity'].str[:-25]\n",
    "    sale_activity = sale_activity.pivot_table(index='Unique Id', values='end_dates', columns='client', aggfunc=max)\n",
    "    \n",
    "    df = df.join(sale_activity, on='Unique Id', how='left').sort_values(by='Unique Id')\n",
    "    \n",
    "    df.columns = list(df.columns[:13]) + [str(col) + '_' + suffix for col in df.columns[13:]]\n",
    "    \n",
    "    return df, [str(col) + '_' + suffix for col in sale_activity.columns]\n",
    "\n",
    "metadata = ['Title', 'Genre', 'Cast Member', 'Year Completed', 'Director',\n",
    "       'Project Type', 'Synopsis', 'Unique Id', 'Website', 'Original Format',\n",
    "       'Dialogue Language', 'Subtitle Language']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:57:35.808082Z",
     "start_time": "2021-01-08T17:57:33.940118Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:190: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:190: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:190: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "svod_avails = pd.read_excel('C:/Users/aleja/Downloads/files/Availability by Region with Reissues - SVOD.xlsx', skiprows=1)\n",
    "svod_avails, svod_sales = cleanup(svod_avails, 'SVOD')\n",
    "\n",
    "ptv_avails = pd.read_excel('C:/Users/aleja/Downloads/files/Availability by Region with Reissues - Basic Pay TV.xlsx', skiprows=1)\n",
    "ptv_avails, ptv_sales = cleanup(ptv_avails, 'PanRegionalPayTV')\n",
    "\n",
    "ptv_local_avails = pd.read_excel('C:/Users/aleja/Downloads/files/Availability by Region with Reissues - Basic Pay TV (Local).xlsx', skiprows=1)\n",
    "ptv_local_avails, ptv_local_sales = cleanup(ptv_local_avails, 'LocalPayTV')\n",
    "\n",
    "ptv_avails = ptv_avails.merge(ptv_local_avails, on=list(ptv_avails.columns[:13]), how='left')\n",
    "\n",
    "merged_df = ptv_avails.merge(svod_avails, on=list(ptv_avails.columns[:13]), how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T23:05:57.972363Z",
     "start_time": "2021-01-07T23:05:57.949423Z"
    }
   },
   "outputs": [],
   "source": [
    "agg_dict = {'Region': lambda x: ' & '.join(x),\n",
    "            'Is Reissue?_PanRegionalPayTV':'first', \n",
    " 'Available?_PanRegionalPayTV': 'first',\n",
    " 'Holdback_PanRegionalPayTV': 'first', \n",
    " 'Note_PanRegionalPayTV': 'first', \n",
    " 'Acq. Expires_PanRegionalPayTV': 'first',\n",
    " 'Previous Sale Activity_PanRegionalPayTV': 'first',\n",
    "'Is Reissue?_LocalPayTV':'first', \n",
    " 'Available?_LocalPayTV': 'first',\n",
    " 'Holdback_LocalPayTV': 'first', \n",
    " 'Note_LocalPayTV': 'first', \n",
    " 'Acq. Expires_LocalPayTV': 'first',\n",
    " 'Previous Sale Activity_LocalPayTV': 'first',\n",
    "'Is Reissue?_SVOD':'first', \n",
    " 'Available?_SVOD': 'first',\n",
    " 'Holdback_SVOD': 'first', \n",
    " 'Note_SVOD': 'first', \n",
    " 'Acq. Expires_SVOD': 'first',\n",
    " 'Previous Sale Activity_SVOD': 'first'}\n",
    "\n",
    "sales = svod_sales+ptv_sales+ptv_local_sales\n",
    "\n",
    "for sale in sales:\n",
    "    agg_dict[sale] = 'first'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T23:05:59.050621Z",
     "start_time": "2021-01-07T23:05:59.042667Z"
    }
   },
   "outputs": [],
   "source": [
    "merged_df['Non-Exclusive Date_PanRegionalPayTV'].fillna(pd.Timestamp.max, inplace=True)\n",
    "merged_df['Exclusive Date_PanRegionalPayTV'].fillna(pd.Timestamp.max, inplace=True)\n",
    "merged_df['Non-Exclusive Date_LocalPayTV'].fillna(pd.Timestamp.max, inplace=True)\n",
    "merged_df['Exclusive Date_LocalPayTV'].fillna(pd.Timestamp.max, inplace=True)\n",
    "merged_df['Non-Exclusive Date_SVOD'].fillna(pd.Timestamp.max, inplace=True)\n",
    "merged_df['Exclusive Date_SVOD'].fillna(pd.Timestamp.max, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T23:06:00.609209Z",
     "start_time": "2021-01-07T23:06:00.525133Z"
    }
   },
   "outputs": [],
   "source": [
    "merged_df = merged_df.groupby(['Unique Id']+[col for col in merged_df.columns if 'Date' in col]).agg(agg_dict).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T23:06:01.169123Z",
     "start_time": "2021-01-07T23:06:01.161115Z"
    }
   },
   "outputs": [],
   "source": [
    "region_mapping = {'Brazil & Latin America excluding Brazil & Mexico & Mexico': 'All Latam', \n",
    "'Brazil & Mexico & Latin America excluding Brazil & Mexico': 'All Latam',\n",
    "'Mexico & Brazil & Latin America excluding Brazil & Mexico': 'All Latam',\n",
    "'Mexico & Latin America excluding Brazil & Mexico & Brazil': 'All Latam',\n",
    "'Latin America excluding Brazil & Mexico & Mexico & Brazil': 'All Latam',\n",
    "'Latin America excluding Brazil & Mexico & Brazil & Mexico': 'All Latam',\n",
    "'Latin America excluding Brazil & Mexico & Mexico': 'Latin America excluding Brazil',\n",
    "'Mexico & Latin America excluding Brazil & Mexico': 'Latin America excluding Brazil', \n",
    "'Latin America excluding Brazil & Mexico & Brazil': 'Latin America excluding Mexico',\n",
    "'Brazil & Latin America excluding Brazil & Mexico': 'Latin America excluding Mexico',\n",
    " 'Brazil': 'Brazil',\n",
    " 'Mexico': 'Mexico',\n",
    " 'Mexico & Brazil': 'Mexico & Brazil',\n",
    " 'Brazil & Mexico': 'Mexico & Brazil',                 \n",
    " 'Latin America excluding Brazil & Mexico': 'Latin America excluding Brazil & Mexico',}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T23:06:02.230902Z",
     "start_time": "2021-01-07T23:06:02.214946Z"
    }
   },
   "outputs": [],
   "source": [
    "merged_df['Region'] = merged_df['Region'].map(region_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T23:06:02.885634Z",
     "start_time": "2021-01-07T23:06:02.842762Z"
    }
   },
   "outputs": [],
   "source": [
    "merged_df = pd.merge(merged_df, ptv_avails[metadata].drop_duplicates(), on='Unique Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T23:06:04.527599Z",
     "start_time": "2021-01-07T23:06:04.037377Z"
    }
   },
   "outputs": [],
   "source": [
    "screeners = pd.read_excel('Z:\\LEDAFILMS\\Alteryx\\Filmtracks\\Project Data ID.xlsx')\n",
    "screeners.dropna(axis=0, subset=['Unique Identifier'], inplace=True)\n",
    "screeners['Unique Id'] = screeners['Unique Identifier'].astype(int)\n",
    "screeners.drop(['Unique Identifier', 'Title', 'Web Site'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T23:06:05.298774Z",
     "start_time": "2021-01-07T23:06:05.265830Z"
    }
   },
   "outputs": [],
   "source": [
    "merged_df = pd.merge(merged_df, screeners, on='Unique Id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T23:06:06.614049Z",
     "start_time": "2021-01-07T23:06:05.898365Z"
    }
   },
   "outputs": [],
   "source": [
    "ratings = pd.read_excel('Z:\\LEDAFILMS\\Alteryx\\Filmtracks\\Ratings & Titles.xls')\n",
    "ratings.dropna(axis=0, subset=['Unique Identifier'], inplace=True)\n",
    "ratings['Unique Id'] = ratings['Unique Identifier'].astype(int)\n",
    "ratings.drop(['Unique Identifier', 'Title', 'Imdb'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T23:06:06.754796Z",
     "start_time": "2021-01-07T23:06:06.740720Z"
    }
   },
   "outputs": [],
   "source": [
    "merged_df = pd.merge(merged_df, ratings, on='Unique Id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T23:06:07.444257Z",
     "start_time": "2021-01-07T23:06:07.429330Z"
    }
   },
   "outputs": [],
   "source": [
    "merged_df.drop(['Acq. Expires_PanRegionalPayTV', 'Acq. Expires_LocalPayTV'], axis=1, inplace=True)\n",
    "merged_df.rename({'Acq. Expires_SVOD': 'Acq. Expires'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T23:06:08.322520Z",
     "start_time": "2021-01-07T23:06:08.308517Z"
    }
   },
   "outputs": [],
   "source": [
    "date_cols = [col for col in merged_df.columns if 'Date' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T23:06:09.023602Z",
     "start_time": "2021-01-07T23:06:09.009676Z"
    }
   },
   "outputs": [],
   "source": [
    "merged_df[date_cols] = merged_df[date_cols].replace({pd.Timestamp.max: pd.NaT})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T23:06:09.995535Z",
     "start_time": "2021-01-07T23:06:09.955614Z"
    }
   },
   "outputs": [],
   "source": [
    "for col in date_cols:\n",
    "    merged_df[col] = merged_df[col].apply(lambda x: x.date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T23:06:10.623640Z",
     "start_time": "2021-01-07T23:06:10.600702Z"
    }
   },
   "outputs": [],
   "source": [
    "cols_ordered = ['Unique Id', 'Title', 'Region', 'Genre', 'Cast Member',\n",
    "       'Year Completed', 'Director', 'Project Type', 'Synopsis', 'Website',\n",
    "       'Original Format', 'Dialogue Language', 'Subtitle Language', 'Non-Exclusive Date_PanRegionalPayTV',\n",
    "       'Exclusive Date_PanRegionalPayTV', 'Non-Exclusive Date_LocalPayTV',\n",
    "       'Exclusive Date_LocalPayTV', 'Non-Exclusive Date_SVOD',\n",
    "       'Exclusive Date_SVOD', 'Is Reissue?_PanRegionalPayTV',\n",
    "       'Available?_PanRegionalPayTV', 'Holdback_PanRegionalPayTV',\n",
    "       'Note_PanRegionalPayTV', 'Previous Sale Activity_PanRegionalPayTV',\n",
    "       'Is Reissue?_LocalPayTV', 'Available?_LocalPayTV',\n",
    "       'Holdback_LocalPayTV', 'Note_LocalPayTV',\n",
    "       'Previous Sale Activity_LocalPayTV', 'Is Reissue?_SVOD',\n",
    "       'Available?_SVOD', 'Holdback_SVOD', 'Note_SVOD', 'Acq. Expires_SVOD',\n",
    "       'Previous Sale Activity_SVOD', 'Link',\n",
    "       'Password', 'US Box Office', 'LATAM Box Office', 'IMDB Link', 'USA ',\n",
    "       'Mexico', 'Brazil', ' Argentina', 'Bolivia', 'Chile', 'Colombia ',\n",
    "       'Costa Rica', 'Ecuador', 'El Salvador', 'Guatemala', 'Honduras',\n",
    "       'Nicaragua', 'Panama', 'Paraguay', 'Peru', 'Dominican Republic',\n",
    "       'Uruguay', 'Venezuela']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T23:06:13.155008Z",
     "start_time": "2021-01-07T23:06:11.353076Z"
    }
   },
   "outputs": [],
   "source": [
    "merged_df[cols_ordered + sales].to_excel('C:/Users/aleja/Documents/Alteryx/Ledafilms Data/Avails/Temp/PayTV-SVOD avails.xlsx',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
