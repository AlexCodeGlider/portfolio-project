{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T18:45:48.867293Z",
     "start_time": "2021-01-09T18:45:48.846383Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "\n",
    "def tidy_split(df, column, sep=',', keep=False):\n",
    "    \"\"\"\n",
    "    Split the values of a column and expand so the new DataFrame has one split\n",
    "    value per row. Filters rows where the column is missing.\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "    df : pandas.DataFrame\n",
    "        dataframe with the column to split and expand\n",
    "    column : str\n",
    "        the column to split and expand\n",
    "    sep : str\n",
    "        the string used to split the column's values\n",
    "    keep : bool\n",
    "        whether to retain the presplit value as it's own row\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Returns a dataframe with the same columns as `df`.\n",
    "    \"\"\"\n",
    "    indexes = list()\n",
    "    new_values = list()\n",
    "    df = df.dropna(subset=[column])\n",
    "    for i, presplit in enumerate(df[column].astype(str)):\n",
    "        values = presplit.split(sep)\n",
    "        if keep and len(values) > 1:\n",
    "            indexes.append(i)\n",
    "            new_values.append(presplit)\n",
    "        for value in values:\n",
    "            indexes.append(i)\n",
    "            new_values.append(value.strip())\n",
    "    new_df = df.iloc[indexes, :].copy()\n",
    "    new_df[column] = new_values\n",
    "    return new_df\n",
    "\n",
    "def cleanup(df, suffix):\n",
    "    df['Non-Exclusive Date'] = df['Non-Exclusive Date'].replace('NOT AVAIL', np.nan).astype('datetime64')\n",
    "    df['Non-Exclusive Date'] = df.apply(lambda x: dt.date.today() if x['Available?'] == 'Avail NE' else x['Non-Exclusive Date'], axis=1)\n",
    "    max_prev_sale_enddate = df['Previous Sale Activity'].str.extractall(r'(\\d{2}[-]\\w{3}[-]\\d{4})').astype('datetime64').reset_index().groupby('level_0')[0].max()\n",
    "    max_prev_sale_enddate = max_prev_sale_enddate + pd.DateOffset(1)\n",
    "    df['max_prev_sale_enddate'] = max_prev_sale_enddate\n",
    "    df['Exclusive Date'] = df['Exclusive Date'].replace(['NOT AVAIL', 'NOT ACQ'], np.nan).astype('datetime64')\n",
    "    max_prev_sale_enddate = df[['Exclusive Date', 'max_prev_sale_enddate']].max(axis=1)\n",
    "    mask = ~df['Exclusive Date'].isna()\n",
    "    df.loc[mask, 'Exclusive Date'] = max_prev_sale_enddate\n",
    "    mask = df['Holdback'] <= dt.datetime.today()\n",
    "    df['Holdback'].loc[mask] = pd.NaT\n",
    "    mask = (df['Non-Exclusive Date'] < df['Exclusive Date']) & (df['Non-Exclusive Date'] > dt.datetime.today())\n",
    "    df['Available?'].loc[mask] = df['Non-Exclusive Date'].loc[mask]\n",
    "    df['First Run or Library'] = df['Is Reissue?'].fillna('First Run')\n",
    "    df['First Run or Library'] = df['First Run or Library'].map({'Yes': 'Library', 'First Run': 'First Run'})\n",
    "    \n",
    "    sale_activity = tidy_split(df, 'Previous Sale Activity', sep='\\n', keep=False)\n",
    "    sale_activity['Previous Sale Activity'] = sale_activity['Previous Sale Activity'].str.replace('.', '')\n",
    "    sale_activity_enddates = sale_activity['Previous Sale Activity'].str[-11:]\n",
    "    date_dict = {'ene': '01', 'feb': '02', 'mar': '03', 'abr': '04', 'may':'05', 'jun': '06', 'jul': '07', 'ago': '08', 'sep':'09', 'oct': '10', 'nov': '11', 'dic': '12'}\n",
    "    sale_activity_enddates = sale_activity_enddates.replace(date_dict, regex=True)\n",
    "    sale_activity_enddates = sale_activity_enddates.replace('own-Unknown', np.nan).astype('datetime64')\n",
    "    sale_activity['end_dates'] = sale_activity_enddates\n",
    "    sale_activity['end_dates'].fillna(sale_activity['Acq. Expires'], inplace=True)\n",
    "    sale_activity['client'] = sale_activity['Previous Sale Activity'].str[:-25]\n",
    "    sale_activity = sale_activity.pivot_table(index='Unique Id', values='end_dates', columns='client', aggfunc=max)\n",
    "    \n",
    "    df = df.join(sale_activity, on='Unique Id', how='left').sort_values(by='Unique Id')\n",
    "    \n",
    "    df.columns = list(df.columns[:13]) + [str(col) + '_' + suffix for col in df.columns[13:]]\n",
    "    \n",
    "    return df, [str(col) + '_' + suffix for col in sale_activity.columns]\n",
    "\n",
    "metadata = ['Title', 'Genre', 'Cast Member', 'Year Completed', 'Director',\n",
    "       'Project Type', 'Synopsis', 'Unique Id', 'Website', 'Original Format',\n",
    "       'Dialogue Language', 'Subtitle Language']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T18:45:54.129813Z",
     "start_time": "2021-01-09T18:45:49.574992Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:190: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:190: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:190: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "svod_avails = pd.read_excel('C:/Users/aleja/Downloads/files/Availability by Region with Reissues - SVOD.xlsx', skiprows=1)\n",
    "svod_avails, svod_sales = cleanup(svod_avails, 'SVOD')\n",
    "\n",
    "ptv_avails = pd.read_excel('C:/Users/aleja/Downloads/files/Availability by Region with Reissues - Basic Pay TV.xlsx', skiprows=1)\n",
    "ptv_avails, ptv_sales = cleanup(ptv_avails, 'PanRegionalPayTV')\n",
    "\n",
    "ptv_local_avails = pd.read_excel('C:/Users/aleja/Downloads/files/Availability by Region with Reissues - Basic Pay TV (Local).xlsx', skiprows=1)\n",
    "ptv_local_avails, ptv_local_sales = cleanup(ptv_local_avails, 'LocalPayTV')\n",
    "\n",
    "ptv_avails = ptv_avails.merge(ptv_local_avails, on=list(ptv_avails.columns[:13]), how='left')\n",
    "\n",
    "merged_df = ptv_avails.merge(svod_avails, on=list(ptv_avails.columns[:13]), how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T18:45:54.285639Z",
     "start_time": "2021-01-09T18:45:54.271975Z"
    }
   },
   "outputs": [],
   "source": [
    "agg_dict = {'Region': lambda x: ' & '.join(x),\n",
    "            'First Run or Library_PanRegionalPayTV':'first', \n",
    " 'Available?_PanRegionalPayTV': 'first',\n",
    " 'Holdback_PanRegionalPayTV': 'first', \n",
    " 'Note_PanRegionalPayTV': 'first', \n",
    " 'Acq. Expires_PanRegionalPayTV': 'first',\n",
    " 'Previous Sale Activity_PanRegionalPayTV': 'first',\n",
    "'First Run or Library_LocalPayTV':'first', \n",
    " 'Available?_LocalPayTV': 'first',\n",
    " 'Holdback_LocalPayTV': 'first', \n",
    " 'Note_LocalPayTV': 'first', \n",
    " 'Acq. Expires_LocalPayTV': 'first',\n",
    " 'Previous Sale Activity_LocalPayTV': 'first',\n",
    "'First Run or Library_SVOD':'first', \n",
    " 'Available?_SVOD': 'first',\n",
    " 'Holdback_SVOD': 'first', \n",
    " 'Note_SVOD': 'first', \n",
    " 'Acq. Expires_SVOD': 'first',\n",
    " 'Previous Sale Activity_SVOD': 'first'}\n",
    "\n",
    "sales = svod_sales+ptv_sales+ptv_local_sales\n",
    "\n",
    "for sale in sales:\n",
    "    agg_dict[sale] = 'first'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T18:45:54.425078Z",
     "start_time": "2021-01-09T18:45:54.411305Z"
    }
   },
   "outputs": [],
   "source": [
    "merged_df['Non-Exclusive Date_PanRegionalPayTV'].fillna(pd.Timestamp.max, inplace=True)\n",
    "merged_df['Exclusive Date_PanRegionalPayTV'].fillna(pd.Timestamp.max, inplace=True)\n",
    "merged_df['Non-Exclusive Date_LocalPayTV'].fillna(pd.Timestamp.max, inplace=True)\n",
    "merged_df['Exclusive Date_LocalPayTV'].fillna(pd.Timestamp.max, inplace=True)\n",
    "merged_df['Non-Exclusive Date_SVOD'].fillna(pd.Timestamp.max, inplace=True)\n",
    "merged_df['Exclusive Date_SVOD'].fillna(pd.Timestamp.max, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T18:45:54.613609Z",
     "start_time": "2021-01-09T18:45:54.551628Z"
    }
   },
   "outputs": [],
   "source": [
    "merged_df = merged_df.groupby(['Unique Id']+[col for col in merged_df.columns if 'Date' in col]).agg(agg_dict).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T18:45:54.754686Z",
     "start_time": "2021-01-09T18:45:54.740274Z"
    }
   },
   "outputs": [],
   "source": [
    "region_mapping = {'Brazil & Latin America excluding Brazil & Mexico & Mexico': 'All Latam', \n",
    "'Brazil & Mexico & Latin America excluding Brazil & Mexico': 'All Latam',\n",
    "'Mexico & Brazil & Latin America excluding Brazil & Mexico': 'All Latam',\n",
    "'Mexico & Latin America excluding Brazil & Mexico & Brazil': 'All Latam',\n",
    "'Latin America excluding Brazil & Mexico & Mexico & Brazil': 'All Latam',\n",
    "'Latin America excluding Brazil & Mexico & Brazil & Mexico': 'All Latam',\n",
    "'Latin America excluding Brazil & Mexico & Mexico': 'Latin America excluding Brazil',\n",
    "'Mexico & Latin America excluding Brazil & Mexico': 'Latin America excluding Brazil', \n",
    "'Latin America excluding Brazil & Mexico & Brazil': 'Latin America excluding Mexico',\n",
    "'Brazil & Latin America excluding Brazil & Mexico': 'Latin America excluding Mexico',\n",
    " 'Brazil': 'Brazil',\n",
    " 'Mexico': 'Mexico',\n",
    " 'Mexico & Brazil': 'Mexico & Brazil',\n",
    " 'Brazil & Mexico': 'Mexico & Brazil',                 \n",
    " 'Latin America excluding Brazil & Mexico': 'Latin America excluding Brazil & Mexico',}\n",
    "\n",
    "merged_df['Region'] = merged_df['Region'].map(region_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T18:45:54.911044Z",
     "start_time": "2021-01-09T18:45:54.881156Z"
    }
   },
   "outputs": [],
   "source": [
    "merged_df = pd.merge(merged_df, ptv_avails[metadata].drop_duplicates(), on='Unique Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T18:45:55.537253Z",
     "start_time": "2021-01-09T18:45:55.038704Z"
    }
   },
   "outputs": [],
   "source": [
    "screeners = pd.read_excel('Z:\\LEDAFILMS\\Alteryx\\Filmtracks\\Project Data ID.xlsx')\n",
    "screeners.dropna(axis=0, subset=['Unique Identifier'], inplace=True)\n",
    "screeners['Unique Id'] = screeners['Unique Identifier'].astype(int)\n",
    "screeners.drop(['Unique Identifier', 'Title', 'Web Site'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T18:45:55.679323Z",
     "start_time": "2021-01-09T18:45:55.664789Z"
    }
   },
   "outputs": [],
   "source": [
    "merged_df = pd.merge(merged_df, screeners, on='Unique Id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T18:45:56.536367Z",
     "start_time": "2021-01-09T18:45:55.805020Z"
    }
   },
   "outputs": [],
   "source": [
    "ratings = pd.read_excel('Z:\\LEDAFILMS\\Alteryx\\Filmtracks\\Ratings & Titles.xls')\n",
    "ratings.dropna(axis=0, subset=['Unique Identifier'], inplace=True)\n",
    "ratings['Unique Id'] = ratings['Unique Identifier'].astype(int)\n",
    "ratings.drop(['Unique Identifier', 'Title', 'Imdb'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T18:45:56.677197Z",
     "start_time": "2021-01-09T18:45:56.664198Z"
    }
   },
   "outputs": [],
   "source": [
    "merged_df = pd.merge(merged_df, ratings, on='Unique Id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T18:45:56.818354Z",
     "start_time": "2021-01-09T18:45:56.806816Z"
    }
   },
   "outputs": [],
   "source": [
    "merged_df.drop(['Acq. Expires_PanRegionalPayTV', 'Acq. Expires_LocalPayTV'], axis=1, inplace=True)\n",
    "merged_df.rename({'Acq. Expires_SVOD': 'Acq. Expires'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T18:45:56.959366Z",
     "start_time": "2021-01-09T18:45:56.944908Z"
    }
   },
   "outputs": [],
   "source": [
    "date_cols = [col for col in merged_df.columns if 'Date' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T18:45:57.101878Z",
     "start_time": "2021-01-09T18:45:57.089023Z"
    }
   },
   "outputs": [],
   "source": [
    "merged_df[date_cols] = merged_df[date_cols].replace({pd.Timestamp.max: pd.NaT})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T18:45:57.259240Z",
     "start_time": "2021-01-09T18:45:57.230504Z"
    }
   },
   "outputs": [],
   "source": [
    "for col in date_cols:\n",
    "    merged_df[col] = merged_df[col].apply(lambda x: x.date())\n",
    "    merged_df[col] = merged_df[col].apply(lambda x: 'Now' if x == dt.date.today() else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T18:45:57.775558Z",
     "start_time": "2021-01-09T18:45:57.761596Z"
    }
   },
   "outputs": [],
   "source": [
    "cols_ordered = ['Project Type', 'Unique Id', 'Title', 'Region', 'Year', 'Genre', 'Cast Member',\n",
    "       'Director',  'Synopsis', 'Website',\n",
    "       'Original Format', 'Dialogue Language', 'Subtitle Language',\n",
    "                \n",
    "                'First Run or Library_PanRegionalPayTV', \n",
    "                'Available?_PanRegionalPayTV', 'Non-Exclusive Date_PanRegionalPayTV',\n",
    "                'Exclusive Date_PanRegionalPayTV', 'Note_PanRegionalPayTV',\n",
    "                'Holdback_PanRegionalPayTV',\n",
    "        \n",
    "                'First Run or Library_LocalPayTV',\n",
    "                'Available?_LocalPayTV', 'Non-Exclusive Date_LocalPayTV',\n",
    "                'Exclusive Date_LocalPayTV', 'Note_LocalPayTV',\n",
    "                'Holdback_LocalPayTV',\n",
    "                \n",
    "                'First Run or Library_SVOD', \n",
    "                'Available?_SVOD', 'Non-Exclusive Date_SVOD', \n",
    "                'Exclusive Date_SVOD', 'Note_SVOD',\n",
    "                'Holdback_SVOD',\n",
    "                \n",
    "                'Acq. Expires',\n",
    "                'Link','Password', 'IMDB Link', 'US Box Office', 'LATAM Box Office', 'USA ',\n",
    "       'Mexico', 'Brazil', ' Argentina', 'Bolivia', 'Chile', 'Colombia ',\n",
    "       'Costa Rica', 'Ecuador', 'El Salvador', 'Guatemala', 'Honduras',\n",
    "       'Nicaragua', 'Panama', 'Paraguay', 'Peru', 'Dominican Republic',\n",
    "       'Uruguay', 'Venezuela']\n",
    "\n",
    "merged_df['Year'] = merged_df['Year Completed']\n",
    "merged_df['Acq. Expires'] = merged_df['Acq. Expires_SVOD'].apply(lambda x: x.date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T18:45:58.773749Z",
     "start_time": "2021-01-09T18:45:58.678892Z"
    }
   },
   "outputs": [],
   "source": [
    "merged_df[sales] = merged_df[sales].apply(lambda x: x.apply(lambda x: x.date()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T18:45:59.383486Z",
     "start_time": "2021-01-09T18:45:59.354536Z"
    }
   },
   "outputs": [],
   "source": [
    "merged_df[[col for col in merged_df.columns if 'Available' in col]] = merged_df[[col for col in merged_df.columns if 'Available' in col]].apply(lambda x: x.apply(lambda x: pd.Timestamp(x) if type(x) == int else x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T18:46:00.024033Z",
     "start_time": "2021-01-09T18:45:59.994113Z"
    }
   },
   "outputs": [],
   "source": [
    "merged_df[[col for col in merged_df.columns if 'Available' in col]] = merged_df[[col for col in merged_df.columns if 'Available' in col]].apply(lambda x: x.apply(lambda x: str(x).replace(' 00:00:00', '')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T18:46:00.680056Z",
     "start_time": "2021-01-09T18:46:00.647114Z"
    }
   },
   "outputs": [],
   "source": [
    "merged_df[[col for col in merged_df.columns if 'Holdback' in col]] = merged_df[[col for col in merged_df.columns if 'Holdback' in col]].apply(lambda x: x.apply(lambda x: str(x).replace(' 00:00:00', '')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T18:46:23.693634Z",
     "start_time": "2021-01-09T18:46:22.955271Z"
    }
   },
   "outputs": [],
   "source": [
    "merged_df[cols_ordered + sales].to_excel('C:/Users/aleja/Documents/Alteryx/Ledafilms Data/Avails/Temp/PayTV-SVOD avails.xlsx',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
